---
layout: post
title: Theory of Variational Autoencoders
date: 2020-11-21
use_math: true
---
The conventional autoencoder is used for reconstructing the input sample. It uses an encoder that projects the data into a low-dimensional space or latent space. A decoder then projects this low-dimensional data back to the original dimension. This set-up is trained in an end-to-end manner to bring reconstructed samples as close to original samples. While just reconstructing the original samples may seem futile, autoencoders can be used for dimensionality reduction. 

Another interesting question is: can autoencoders be used to generate the new samples? For example, once the training is completed, can the a random vector be passed to the decoder to reconstruct some new sample that are not present in the training data? The answer most of the time in a NO. If the random vector passed to the decoder does not belongs to the embedding learn through the training data, then decoder will output some random noise. This is because there is no constrain on the latent space distribution. Hence, any random distribution will be learned as long as reconstruction loss in minimized. The new random vector passed to the decoder may not belong to this learned distribution, and hence the output will be random. The immediate solution is to put some constrain on the latent space distribution. 
\begin{equation}
L=||x-\hat{x}||^2+D_{KL}(p(z|x)||q(z)),
\end{equation}
where, the $D_{KL}$ (KL divergence) quantify the difference between the two distributions. As can be seen from this loss function, we are still minimizing the reconstruction loss, $D_{KL}$  is acting as a regularization so that latent space distribution $(p(z|X))$ is bought close to some predefined distribution $(q(z))$. Once the training is completed, we can sample from the $q(z)$ to generate the new samples. 
